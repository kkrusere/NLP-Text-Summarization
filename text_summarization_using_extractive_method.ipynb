{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujwb2FAFcecN"
      },
      "source": [
        "## <center>**NLP Text Summarization** </center>\n",
        "**<center>Extractive Summarization</center>**\n",
        "<center><em>\n",
        "Text summarization refers to the technique of shortening long pieces of text, with the intention of creating a coherent and fluent summary having only the main points outlined in the document. Basically, the process of creating shorter text without removing the semantic structure of text.\n",
        "</em></center>\n",
        "<br>\n",
        "<center><img src=\"https://github.com/kkrusere/NLP-Text-Summarization/blob/main/assets/mchinelearning_text_sum.png?raw=1\" width=600/></center>\n",
        "\n",
        "***Project Contributors:*** Kuzi Rusere<br>\n",
        "**MVP streamlit App URL:** N/A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S77AYwGcI7lu"
      },
      "source": [
        "**Extractive summarization** focuses on picking out the most essential phrases and sentences directly from the original text and putting them together to create a shorter version. The original text is not changed in any way. The strength of this method is its accuracy; because it uses the original words, it's very reliable for keeping facts straight."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQJ5m19AcecP"
      },
      "source": [
        "For our example text, we are going use this brief explainer of the history of Chaos theory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yXjD8jTgcecP"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "In 1961, a meteorologist by the name of Edward Lorenz made a profound discovery. Lorenz was utilising the new-found power of computers in an attempt to more accurately predict the weather. He created a mathematical model which, when supplied with a set of numbers representing the current weather, could predict the weather a few minutes in advance.\n",
        "Once this computer program was up and running, Lorenz could produce long-term forecasts by feeding the predicted weather back into the computer over and over again, with each run forecasting further into the future.Accurate minute-by-minute forecasts added up into days, and then weeks.\n",
        "One day, Lorenz decided to rerun one of his forecasts. In the interests of saving time he decided not to start from scratch; instead he took the computer’s prediction from halfway through the first run and used that as the starting point.\n",
        "After a well-earned coffee break, he returned to discover something unexpected. Although the computer’s new predictions started out the same as before, the two sets of predictions soon began diverging drastically. What had gone wrong?\n",
        "Lorenz soon realised that while the computer was printing out the predictions to three decimal places, it was actually crunching the numbers internally using six decimal places.\n",
        "So while Lorenz had started the second run with the number 0.506, the original run had used the number 0.506127.\n",
        "A difference of one part in a thousand: the same sort of difference that a flap of a butterfly’s wing might make to the breeze on your face. The starting weather conditions had been virtually identical. The two predictions were anything but.\n",
        "Lorenz had found the seeds of chaos. In systems that behave nicely - without chaotic effects - small differences only produce small effects. In this case, Lorenz’s equations were causing errors to steadily grow over time.\n",
        "This meant that tiny errors in the measurement of the current weather would not stay tiny, but relentlessly increased in size each time they were fed back into the computer until they had completely swamped the predictions.\n",
        "Lorenz famously illustrated this effect with the analogy of a butterfly flapping its wings and thereby causing the formation of a hurricane half a world away.\n",
        "A nice way to see this “butterfly effect” for yourself is with a game of pool or billiards. No matter how consistent you are with the first shot (the break), the smallest of differences in the speed and angle with which you strike the white ball will cause the pack of billiards to scatter in wildly different directions every time.\n",
        "The smallest of differences are producing large effects - the hallmark of a chaotic system.\n",
        "It is worth noting that the laws of physics that determine how the billiard balls move are precise and unambiguous: they allow no room for randomness.\n",
        "What at first glance appears to be random behaviour is completely deterministic - it only seems random because imperceptible changes are making all the difference.\n",
        "The rate at which these tiny differences stack up provides each chaotic system with a prediction horizon - a length of time beyond which we can no longer accurately forecast its behaviour.\n",
        "In the case of the weather, the prediction horizon is nowadays about one week (thanks to ever-improving measuring instruments and models).\n",
        "Some 50 years ago it was 18 hours. Two weeks is believed to be the limit we could ever achieve however much better computers and software get.\n",
        "Surprisingly, the solar system is a chaotic system too - with a prediction horizon of a hundred million years. It was the first chaotic system to be discovered, long before there was a Chaos Theory.\n",
        "In 1887, the French mathematician Henri Poincaré showed that while Newton’s theory of gravity could perfectly predict how two planetary bodies would orbit under their mutual attraction, adding a third body to the mix rendered the equations unsolvable.\n",
        "The best we can do for three bodies is to predict their movements moment by moment, and feed those predictions back into our equations …\n",
        "Though the dance of the planets has a lengthy prediction horizon, the effects of chaos cannot be ignored, for the intricate interplay of gravitation tugs among the planets has a large influence on the trajectories of the asteroids.\n",
        "Keeping an eye on the asteroids is difficult but worthwhile, since such chaotic effects may one day fling an unwelcome surprise our way.\n",
        "On the flip side, they can also divert external surprises such as steering comets away from a potential collision with Earth.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mr0rS_hwI7lx"
      },
      "source": [
        "#### Key Steps in Extractive Summarization:\n",
        "\n",
        "1. **Text Preprocessing:** Cleaning and preprocessing the text (e.g., remove stopwords, punctuation, and lowercasing).\n",
        "2. **Sentence Tokenization:** Splitting the text into individual sentences.\n",
        "3. **Word Tokenization and Normalization:** Tokenizing sentences into words and apply normalization techniques like stemming or lemmatization.\n",
        "4. **Scoring Sentences:** Assigning a score to each sentence based on different features (e.g., word frequency, sentence position, presence of keywords).\n",
        "5. **Rank and Sentence Selection:** Ranking sentences based on their scores and select the top-ranked ones for the summary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s16Cp0KJI7lx"
      },
      "source": [
        "**First What is NLTK in Natural Language Processing (NLP)?**\n",
        "\n",
        "NLTK, or Natural Language Toolkit, is a powerful and widely-used Python library for working with human language data in the field of Natural Language Processing (NLP).\n",
        "\n",
        "\n",
        "* Core Functionality: NLTK provides a comprehensive suite of tools and resources for various NLP tasks including:\n",
        "\n",
        "    - Text preprocessing: Tokenization, stemming, lemmatization, stop word removal, part-of-speech tagging\n",
        "    - Corpus access: Interfaces to numerous text corpora and lexical resources\n",
        "    - Text analysis: Concordance, frequency distribution, collocations, and more\n",
        "    - Machine learning: Classification, named entity recognition, and other NLP applications\n",
        "\n",
        "* Benefits:\n",
        "\n",
        "    - Ease of use: NLTK's user-friendly interface and extensive documentation make it accessible for both beginners and experienced NLP practitioners.\n",
        "    - Versatility: The library supports a wide range of NLP tasks and techniques, enabling flexibility in project development.\n",
        "    - Community and Resources: NLTK boasts a large and active community, providing ample support and resources for learning and troubleshooting.\n",
        "\n",
        "* Applications: NLTK is employed in various NLP projects, including:\n",
        "\n",
        "    - Sentiment analysis\n",
        "    - Chatbots and conversational agents\n",
        "    - Text summarization\n",
        "    - Machine translation\n",
        "    - Information extraction\n",
        "\n",
        "\n",
        "In essence, NLTK is a foundational tool in the NLP landscape, offering a rich set of functionalities and resources to facilitate the development of diverse natural language processing applications.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16hO50WsI7ly",
        "outputId": "0a78993f-9a5a-4126-b543-420035cc7206"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiDRjZRfI7ly"
      },
      "source": [
        "\n",
        "1. Preprocessing the Text:\n",
        "    - The text is tokenized into sentences and words.\n",
        "    - Stop_words (common words like \"the,\" \"is,\" etc.) and punctuation are removed to focus on meaningful words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "R0ZboGgyI7lz"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text, stop_words = stop_words):\n",
        "    # Tokenize text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Tokenize sentences into words\n",
        "    words = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove stopwords and punctuation\n",
        "    filtered_words = [word for word in words if word not in stop_words and word not in string.punctuation]\n",
        "\n",
        "    return sentences, filtered_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBiicpBfI7lz"
      },
      "source": [
        "2. Word Frequency Calculation:\n",
        "- A frequency distribution of the filtered words is calculated to determine the importance of each word.\n",
        "- Frequencies are normalized to bring them to a common scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NPpj-vhYI7l0"
      },
      "outputs": [],
      "source": [
        "def compute_word_frequency(filtered_words):\n",
        "    # Frequency distribution of words\n",
        "    freq_dist = FreqDist(filtered_words)\n",
        "\n",
        "    # Normalize frequencies\n",
        "    max_freq = max(freq_dist.values())\n",
        "    for word in freq_dist.keys():\n",
        "        freq_dist[word] = (freq_dist[word] / max_freq)\n",
        "\n",
        "    return freq_dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbpHtzsGI7l0"
      },
      "source": [
        "3. Sentence Scoring:\n",
        "- Each sentence is scored based on the sum of the frequencies of the words it contains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7CQTPn0zI7l0"
      },
      "outputs": [],
      "source": [
        "def score_sentences(sentences, freq_dist):\n",
        "    # Score each sentence based on the frequency of words it contains\n",
        "    sentence_scores = {}\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence.lower())\n",
        "        score = 0\n",
        "        for word in words:\n",
        "            if word in freq_dist:\n",
        "                score += freq_dist[word]\n",
        "        sentence_scores[sentence] = score\n",
        "\n",
        "    return sentence_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8yR9BFnI7l0"
      },
      "source": [
        "4. Generating the Summary:\n",
        "- Sentences are sorted based on their scores, and the top sentences are selected to form the summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "M3zapV7CI7l1"
      },
      "outputs": [],
      "source": [
        "def generate_summary(sentence_scores, num_sentences=3):\n",
        "    # Sort sentences by their scores in descending order\n",
        "    sorted_sentences = sorted(sentence_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Select the top 'num_sentences' sentences\n",
        "    summary_sentences = [sentence[0] for sentence in sorted_sentences[:num_sentences]]\n",
        "\n",
        "    # Join the selected sentences to form the summary\n",
        "    summary = ' '.join(summary_sentences)\n",
        "\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaCDZcB8I7l1",
        "outputId": "6cb439e2-cee1-4a9f-9c2b-3d3485fcbba6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary:\n",
            "\n",
            "The best we can do for three bodies is to predict their movements moment by moment, and feed those predictions back into our equations …\n",
            "Though the dance of the planets has a lengthy prediction horizon, the effects of chaos cannot be ignored, for the intricate interplay of gravitation tugs among the planets has a large influence on the trajectories of the asteroids. Once this computer program was up and running, Lorenz could produce long-term forecasts by feeding the predicted weather back into the computer over and over again, with each run forecasting further into the future.Accurate minute-by-minute forecasts added up into days, and then weeks. This meant that tiny errors in the measurement of the current weather would not stay tiny, but relentlessly increased in size each time they were fed back into the computer until they had completely swamped the predictions.\n"
          ]
        }
      ],
      "source": [
        "# Execute the extractive summarization steps\n",
        "sentences, filtered_words = preprocess_text(text)\n",
        "freq_dist = compute_word_frequency(filtered_words)\n",
        "sentence_scores = score_sentences(sentences, freq_dist)\n",
        "summary = generate_summary(sentence_scores)\n",
        "\n",
        "# Output the summary\n",
        "print(\"Summary:\\n\")\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YfqDuQy-I7l3"
      },
      "outputs": [],
      "source": [
        "# \"\"\"Summary:\n",
        "\n",
        "# The best we can do for three bodies is to predict their movements moment by moment, and feed those predictions back into our equations …\n",
        "# Though the dance of the planets has a lengthy prediction horizon, the effects of chaos cannot be ignored, for the intricate interplay of\n",
        "# gravitation tugs among the planets has a large influence on the trajectories of the asteroids. Once this computer program was up and running,\n",
        "# Lorenz could produce long-term forecasts by feeding the predicted weather back into the computer over and over again, with each run forecasting\n",
        "# further into the future.Accurate minute-by-minute forecasts added up into days, and then weeks. This meant that tiny errors in the measurement\n",
        "# of the current weather would not stay tiny, but relentlessly increased in size each time they were fed back into the computer until they had\n",
        "# completely swamped the predictions.\n",
        "# \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZU7hKlLI7l3"
      },
      "source": [
        "The output summary is not a good summary of the original text.\n",
        "\n",
        "Here's why:\n",
        "\n",
        "- **Focus on a Specific Example:** The summary primarily focuses on the specific example of Lorenz's weather prediction model. While this is a crucial part of the text, it neglects the broader concept of chaos theory and its implications in other systems.\n",
        "- **The summary fails to mention key ideas like:**\n",
        "    - **Deterministic Chaos:** The idea that seemingly random behavior can arise from deterministic systems due to sensitivity to initial conditions.\n",
        "    - **Prediction Horizon:** The time limit beyond which accurate predictions become impossible in a chaotic system.\n",
        "    - **Chaos in the Solar System:** The chaotic nature of the solar system and its implications for asteroid trajectories.\n",
        "- **Lacks Context and Flow:** The summary feels disjointed, jumping straight into Lorenz's experiment without providing sufficient context about chaos theory and its significance.\n",
        "\n",
        "The summary provides a narrow view by focusing on a single example and neglecting the broader concepts and implications of chaos theory explored in the original text. There is need to improve the summarization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dlRuAbUVI7l3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtyLaJJQI7l3"
      },
      "source": [
        "##### Let's get into how we can Improve the Scoring Mechanism\n",
        "- To improve the extractive summarization approach, we can incorporate additional features to score sentences more accurately. Here are some ways to enhance the scoring mechanism:\n",
        "\n",
        "Additional Features for Scoring:\n",
        "\n",
        "1. Sentence Length:\n",
        "    - Avoiding very short or too long sentences, which may be less informative or difficult to understand.\n",
        "    - We can add a penalty or a boost to the sentence score based on its length.\n",
        "2. Sentence Position:\n",
        "    - In many texts (e.g., news articles), important information is often found in the first few sentences.\n",
        "    - We can give a higher score to sentences appearing earlier in the text.\n",
        "3. Named Entity Recognition (NER):\n",
        "    - Sentences containing named entities (like people, places, organizations) may carry more important information.\n",
        "    - We can boost the score of sentences that contain named entities.\n",
        "4. Thematic Words:\n",
        "    - Words that appear frequently throughout the text are often central to the main idea.\n",
        "    - We can further emphasize the sentences containing these thematic words.\n",
        "\n",
        "\n",
        "\n",
        "Let's incorporate these features into the revised version of extractive summarization code using NLTK and spaCy\n",
        "\n",
        "**What is spaCy in Natural Language Processing (NLP)?**\n",
        "\n",
        "\n",
        "**spaCy** is an open-source Python library specifically designed for production/enterprise-grade Natural Language Processing (NLP). It focuses on providing fast and efficient tools for various NLP tasks, making it well-suited for real-world applications and large-scale text processing.\n",
        "\n",
        "\n",
        "- Key Features and Benefits of spaCy:\n",
        "\n",
        "    * Speed and Efficiency: spaCy is known for its exceptional performance, making it ideal for handling large volumes of text data. It leverages optimized algorithms and Cython implementations for blazing-fast processing.\n",
        "    * Production-Ready: spaCy is built with a focus on practical applications and ease of deployment. Its streamlined API and well-documented functionalities simplify the development process, enabling seamless integration into existing workflows.\n",
        "    * Pre-trained Models: spaCy offers a range of pre-trained statistical models for several languages, enabling out-of-the-box capabilities such as part-of-speech tagging, named entity recognition, dependency parsing, and more.\n",
        "    * Customizability: In addition to offering pre-trained models, spaCy allows developers to train custom models for specific tasks or domains, enhancing its flexibility for specialized applications.\n",
        "    * Ecosystem and Integrations: spaCy benefits from a rich ecosystem of plugins and extensions, enabling seamless integration with other libraries and tools within the NLP and machine learning landscape.\n",
        "\n",
        "- Typical Use Cases:\n",
        "\n",
        "    * Information Extraction: spaCy excels at tasks like entity recognition, relation extraction, and fact extraction, enabling the automatic extraction of valuable insights from unstructured text data.\n",
        "    * Text Classification: spaCy can be used for sentiment analysis, topic classification, and other text classification tasks, helping categorize and understand large amounts of text.\n",
        "    * Chatbots and Conversational Agents: spaCy's capabilities can be leveraged to power chatbots and conversational agents by providing text preprocessing, intent recognition, and entity extraction functionalities.\n",
        "    * Preprocessing for Deep Learning: spaCy's efficient text preprocessing tools, including tokenization, lemmatization, and dependency parsing, make it a valuable asset for preparing text data for further analysis using deep learning models.\n",
        "\n",
        "\n",
        "\n",
        "**spaCy** is a powerful and versatile NLP library, renowned for its speed, efficiency, and production-readiness. Its wide range of features and capabilities makes it an indispensable tool for developers and practitioners working on real-world NLP applications and projects involving large-scale text data processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4sH65dH0I7l4"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "# Load the spaCy model for Named Entity Recognition (NER)\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gXdshWtZI7l4"
      },
      "outputs": [],
      "source": [
        "def score_sentences(sentences, freq_dist, text):\n",
        "    \"\"\"\n",
        "    Scores sentences based on multiple features to determine their importance for extractive summarization.\n",
        "\n",
        "    This function computes a score for each sentence in the text by considering various features like word frequency,\n",
        "    sentence length, sentence position, and the presence of named entities. The score is used to rank sentences\n",
        "    when generating an extractive summary.\n",
        "\n",
        "    Args:\n",
        "        sentences (list of str): A list of sentences extracted from the original text.\n",
        "        freq_dist (nltk.probability.FreqDist): A frequency distribution object containing normalized word frequencies.\n",
        "        text (str): The original text from which sentences are extracted.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary where keys are sentences and values are their corresponding scores.\n",
        "    \"\"\"\n",
        "    sentence_scores = {}  # Initialize an empty dictionary to store scores for each sentence\n",
        "    doc = nlp(text)  # Use spaCy's NLP model to analyze the text for Named Entity Recognition (NER)\n",
        "\n",
        "    for idx, sentence in enumerate(sentences):\n",
        "        words = word_tokenize(sentence.lower())  # Tokenize the sentence into words and convert to lowercase\n",
        "        score = 0  # Initialize the score for the current sentence\n",
        "\n",
        "        # Score based on word frequency\n",
        "        for word in words:\n",
        "            if word in freq_dist:  # If the word is in the frequency distribution\n",
        "                score += freq_dist[word]  # Add its frequency to the score\n",
        "\n",
        "        # Feature 1: Sentence Length\n",
        "        if len(words) > 4 and len(words) < 25:  # Length threshold for a sentence to be considered \"ideal\"\n",
        "            score *= 1.2  # Boost score if the sentence length is within the desired range\n",
        "\n",
        "        # Feature 2: Sentence Position\n",
        "        if idx < len(sentences) * 0.2:  # If the sentence is among the first 20% in the text\n",
        "            score *= 1.5  # Boost score to emphasize its importance\n",
        "\n",
        "        # Feature 3: Named Entity Recognition (NER)\n",
        "        named_entities = [ent.text for ent in doc.ents if ent.text in sentence]  # Extract named entities from the sentence\n",
        "        if named_entities:  # If the sentence contains named entities\n",
        "            score *= 1.3  # Boost score\n",
        "\n",
        "        sentence_scores[sentence] = score  # Store the computed score for the current sentence\n",
        "\n",
        "    return sentence_scores  # Return the dictionary containing sentences and their scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "m4GBfX9mI7l5"
      },
      "outputs": [],
      "source": [
        "def generate_summary(sentence_scores, num_sentences=3):\n",
        "    sorted_sentences = sorted(sentence_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    summary_sentences = [sentence[0] for sentence in sorted_sentences[:num_sentences]]\n",
        "    summary = ' '.join(summary_sentences)\n",
        "    return summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMax50uWI7l5",
        "outputId": "29cf37a6-1c8b-4d9b-ab43-be5ed4c1a0fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enhanced/improved Summary:\n",
            "Once this computer program was up and running, Lorenz could produce long-term forecasts by feeding the predicted weather back into the computer over and over again, with each run forecasting further into the future.Accurate minute-by-minute forecasts added up into days, and then weeks. In the interests of saving time he decided not to start from scratch; instead he took the computer’s prediction from halfway through the first run and used that as the starting point. The best we can do for three bodies is to predict their movements moment by moment, and feed those predictions back into our equations …\n",
            "Though the dance of the planets has a lengthy prediction horizon, the effects of chaos cannot be ignored, for the intricate interplay of gravitation tugs among the planets has a large influence on the trajectories of the asteroids.\n"
          ]
        }
      ],
      "source": [
        "sentences, filtered_words = preprocess_text(text)\n",
        "freq_dist = compute_word_frequency(filtered_words)\n",
        "sentence_scores = score_sentences(sentences, freq_dist, text)\n",
        "summary = generate_summary(sentence_scores)\n",
        "\n",
        "print(\"Enhanced/improved Summary:\")\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "r7LJX5xPI7l6",
        "outputId": "b3774c08-5570-42a7-e8b4-90212b6c33bd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Enhanced/improved Summary:\\n    Once this computer program was up and running, Lorenz could produce long-term forecasts by feeding the predicted weather back into the computer over and over again, with\\n    each run forecasting further into the future.Accurate minute-by-minute forecasts added up into days, and then weeks. In the interests of saving time he decided not to start\\n    from scratch; instead he took the computer’s prediction from halfway through the first run and used that as the starting point. The best we can do for three bodies is to\\n    predict their movements moment by moment, and feed those predictions back into our equations …\\n\\n    Though the dance of the planets has a lengthy prediction horizon, the effects of chaos cannot be ignored, for the intricate interplay of gravitation tugs among the planets\\n    has a large influence on the trajectories of the asteroids.'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"Enhanced/improved Summary:\n",
        "    Once this computer program was up and running, Lorenz could produce long-term forecasts by feeding the predicted weather back into the computer over and over again, with\n",
        "    each run forecasting further into the future.Accurate minute-by-minute forecasts added up into days, and then weeks. In the interests of saving time he decided not to start\n",
        "    from scratch; instead he took the computer’s prediction from halfway through the first run and used that as the starting point. The best we can do for three bodies is to\n",
        "    predict their movements moment by moment, and feed those predictions back into our equations …\n",
        "\n",
        "    Though the dance of the planets has a lengthy prediction horizon, the effects of chaos cannot be ignored, for the intricate interplay of gravitation tugs among the planets\n",
        "    has a large influence on the trajectories of the asteroids.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79ZIcRD1I7l6"
      },
      "source": [
        "The enhanced/improved summary is better than the previous one, but it still has some shortcomings:\n",
        "\n",
        "- Positives:\n",
        "\n",
        "    * Includes more context: It now mentions Lorenz and his experiment with weather prediction, providing some context for the discussion of chaos theory.\n",
        "    * Slightly better flow: The sentences are somewhat more connected than in the previous summary.\n",
        "\n",
        "- Negatives:\n",
        "\n",
        "    * Still incomplete: While it touches on Lorenz's experiment and the three-body problem, it still misses the core concept of chaos theory – the idea that tiny differences in initial conditions can lead to drastically different outcomes over time. The terms \"butterfly effect\" and \"prediction horizon\" are still absent.\n",
        "    * Lacks focus: The summary jumps between weather prediction, the three-body problem, and the solar system without clearly connecting these ideas.\n",
        "    * Some sentences are still out of context: The sentence \"In the interests of saving time he decided not to start from scratch; instead he took the computer’s prediction from halfway through the first run and used that as the starting point\" is included without explaining its significance in the context of Lorenz's discovery.\n",
        "\n",
        "\n",
        "Overall, the enhanced summary is an improvement, but it still needs work to be considered a good summary of the original text. It needs to be more focused, include the key concepts of chaos theory, and provide a clear and logical flow of ideas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "SFJ3Gx4HI7l6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bh7_Bx53I7l6"
      },
      "source": [
        "Lets incooporate some Evaluation:\n",
        "\n",
        "To evaluate the quality of the summaries generated by our extractive summarization algorithm, we can use several evaluation metrics. The most widely used evaluation metric for summarization tasks is ROUGE (Recall-Oriented Understudy for Gisting Evaluation). Other evaluation metrics include BLEU (Bilingual Evaluation Understudy) and METEOR (Metric for Evaluation of Translation with Explicit ORdering).\n",
        "\n",
        "Evaluation Metrics for Summarization\n",
        "- ROUGE (Recall-Oriented Understudy for Gisting Evaluation):\n",
        "    * ROUGE-1: Measures the overlap of unigrams (single words) between the generated summary and a reference summary.\n",
        "    * ROUGE-2: Measures the overlap of bigrams (two consecutive words).\n",
        "    * ROUGE-L: Measures the longest common subsequence (LCS) between the generated and reference summaries, capturing the in-sequence overlap.\n",
        "- BLEU (Bilingual Evaluation Understudy):\n",
        "    * Primarily used for machine translation but can be adapted for summarization.\n",
        "    * Measures n-gram precision of a generated text concerning one or more reference texts.\n",
        "- METEOR (Metric for Evaluation of Translation with Explicit ORdering):\n",
        "    * Designed to improve BLEU by addressing problems like synonymy and stemming.\n",
        "    * It considers unigram matches between generated and reference summaries, applying stemming and synonymy matching.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPpCD_EyO2XE",
        "outputId": "93bc26a8-0a7b-4aa0-8e2b-2a5b6309f444"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge-score\n",
        "\n",
        "from rouge_score import rouge_scorer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "u0fbEKNYR57z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "IzciZplkRpgB"
      },
      "outputs": [],
      "source": [
        "def calculate_rouge_scores1(generated_summary, reference_summary):\n",
        "    \"\"\"\n",
        "    Calculate ROUGE scores for a generated summary compared to a reference summary.\n",
        "\n",
        "    Args:\n",
        "        generated_summary (str): The generated summary to evaluate.\n",
        "        reference_summary (str): The reference summary for comparison.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing ROUGE-1, ROUGE-2, and ROUGE-L scores.\n",
        "    \"\"\"\n",
        "    # Initialize the ROUGE scorer with ROUGE-1, ROUGE-2, and ROUGE-L\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    # Calculate ROUGE scores\n",
        "    scores = scorer.score(reference_summary, generated_summary)\n",
        "\n",
        "    # Print the ROUGE scores\n",
        "    print(\"ROUGE Scores:\")\n",
        "    print(f\"ROUGE-1: {scores['rouge1'].precision:.3f}\")\n",
        "    print(f\"ROUGE-2: {scores['rouge2'].precision:.3f}\")\n",
        "    print(f\"ROUGE-L: {scores['rougeL'].precision:.3f}\")\n",
        "\n",
        "    return scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBSkH9RMRxGt",
        "outputId": "53aee96a-6b8a-42e1-c6b2-d7ef1f16f07a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE Scores:\n",
            "ROUGE-1: 0.394\n",
            "ROUGE-2: 0.050\n",
            "ROUGE-L: 0.190\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'rouge1': Score(precision=0.39436619718309857, recall=0.30601092896174864, fmeasure=0.3446153846153846),\n",
              " 'rouge2': Score(precision=0.04964539007092199, recall=0.038461538461538464, fmeasure=0.043343653250773995),\n",
              " 'rougeL': Score(precision=0.19014084507042253, recall=0.14754098360655737, fmeasure=0.16615384615384615)}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reference_summary = \"\"\"Chaos theory is a field of study in mathematics that examines the behavior of dynamical systems that are highly sensitive to initial conditions.\n",
        "This sensitivity is popularly referred to as the butterfly effect, where a small change in one state of a deterministic nonlinear system can result in large differences in a later state.\n",
        "Edward Lorenz, a meteorologist, made significant contributions to chaos theory through his work on weather prediction models. He discovered that even tiny errors in the initial\n",
        "measurements of weather conditions could lead to drastically different forecasts over time. This finding highlighted the inherent limitations in predicting the long-term behavior of\n",
        "chaotic systems. The concept of a prediction horizon emerged, representing the time limit beyond which accurate predictions become impossible due to the exponential growth of errors.\n",
        "Chaos theory has implications beyond weather forecasting. For instance, the three-body problem in celestial mechanics demonstrates the chaotic nature of gravitational interactions\n",
        "between three or more celestial bodies. Even with precise initial conditions, predicting the long-term trajectories of these bodies becomes increasingly difficult due to the\n",
        "sensitivity to initial conditions.\"\"\"\n",
        "\n",
        "# Calculate and display ROUGE scores\n",
        "calculate_rouge_scores1(summary, reference_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "bQj87I3aPRnj"
      },
      "outputs": [],
      "source": [
        "def calculate_rouge_scores2(generated_summary, original_text):\n",
        "    \"\"\"\n",
        "    Evaluates the quality of a generated summary using ROUGE scores, even without a reference summary.\n",
        "\n",
        "    This function calculates ROUGE scores by comparing the generated summary against the original text,\n",
        "    serving as a pseudo-reference. While not ideal, it provides a relative measure of how well the\n",
        "    summary captures the salient information from the original text.\n",
        "\n",
        "    Args:\n",
        "        generated_summary (str): The summary generated by the summarization algorithm.\n",
        "        original_text (str): The original text from which the summary was generated.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing ROUGE-1, ROUGE-2, and ROUGE-L scores.\n",
        "    \"\"\"\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    # Calculate ROUGE scores\n",
        "    scores = scorer.score(original_text, generated_summary)\n",
        "\n",
        "    # Print the ROUGE scores\n",
        "    print(\"ROUGE Scores:\")\n",
        "    print(f\"ROUGE-1: {scores['rouge1'].precision:.3f}\")\n",
        "    print(f\"ROUGE-2: {scores['rouge2'].precision:.3f}\")\n",
        "    print(f\"ROUGE-L: {scores['rougeL'].precision:.3f}\")\n",
        "    return scores\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3k0FxBfjPRku",
        "outputId": "aa52ed19-7117-4e37-c1b6-d26346b0fde5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE Scores:\n",
            "ROUGE-1: 1.000\n",
            "ROUGE-2: 0.986\n",
            "ROUGE-L: 1.000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'rouge1': Score(precision=1.0, recall=0.18733509234828497, fmeasure=0.31555555555555553),\n",
              " 'rouge2': Score(precision=0.9858156028368794, recall=0.18361955085865259, fmeasure=0.30957683741648107),\n",
              " 'rougeL': Score(precision=1.0, recall=0.18733509234828497, fmeasure=0.31555555555555553)}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Evaluate the generated summary\n",
        "calculate_rouge_scores1(summary, text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xfKR0b1JUKk0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRzuLJ1xZedk"
      },
      "source": [
        "Let's analyze the results of the two functions, `calculate_rouge_scores1` and `calculate_rouge_scores2`, which calculate ROUGE scores for generated summaries in different contexts.\n",
        "\n",
        "- Explanation of ROUGE Scores, The ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metric evaluates the quality of a summary by comparing it to a reference (or original) text. ROUGE scores are usually presented in three parts:\n",
        "\n",
        "> - Precision: Measures the percentage of words in the generated summary that appear in the reference summary.\n",
        "> - Recall: Measures the percentage of words in the reference summary that appear in the generated summary.\n",
        "> - F-Measure (F1 Score): The harmonic mean of precision and recall, providing a single score that balances both.\n",
        "\n",
        "1. `calculate_rouge_scores1` Results:\n",
        "\n",
        "> - The `calculate_rouge_scores1` function computes ROUGE scores between a generated summary and a human-written reference summary. The output of the function is:\n",
        "\n",
        "> - ```yaml\n",
        "    ROUGE Scores:\n",
        "    ROUGE-1: 0.394\n",
        "    ROUGE-2: 0.050\n",
        "    ROUGE-L: 0.190\n",
        "\n",
        "\n",
        "*  Explanation:\n",
        "    - `ROUGE-1 (Unigram Precision: 0.394)`: Approximately 39.4% of the unigrams (individual words) in the generated summary are also present in the reference summary.\n",
        "    - `ROUGE-2 (Bigram Precision: 0.050)`: Only about 5% of the bigrams (two consecutive words) match between the generated summary and the reference summary. This low score suggests that the generated summary does not capture many consecutive word pairs from the reference.\n",
        "    - `ROUGE-L (Longest Common Subsequence Precision: 0.190)`: The LCS (Longest Common Subsequence) precision score is about 19%. This suggests that the generated summary captures some sequences of words from the reference summary, but the overlap is not substantial.\n",
        "\n",
        "*  Summary:\n",
        "    - These scores indicate that the generated summary only partially aligns with the reference summary.\n",
        "    - The relatively low ROUGE-2 and ROUGE-L scores suggest that the generated summary does not capture the detailed structure or phrasing of the reference text very well.\n",
        "    - The summary may contain the key points but not with the same level of detail or in the same sequence.\n",
        "\n",
        "\n",
        "2. `calculate_rouge_scores2 Results:`\n",
        "\n",
        "> - The `calculate_rouge_scores2` function computes ROUGE scores by comparing the generated summary to the original text (from which the summary was generated). This provides an evaluation of how well the generated summary captures the content of the original text. The output of the function is:\n",
        "\n",
        "> - ```yaml\n",
        "    ROUGE Scores:\n",
        "    ROUGE-1: 1.000\n",
        "    ROUGE-2: 0.986\n",
        "    ROUGE-L: 1.000\n",
        "\n",
        "*  Explanation:\n",
        "    - `ROUGE-1 (Unigram Precision: 1.000)`: The generated summary contains all the unigrams found in the original text. However, this is because the generated summary only contains words that also appear in the original text.\n",
        "    - `ROUGE-2 (Bigram Precision: 0.986)`: The bigram precision is very high (98.6%), indicating that nearly all consecutive word pairs in the generated summary appear in the original text.\n",
        "    - `ROUGE-L (Longest Common Subsequence Precision: 1.000)`: The LCS precision score is 1.0, meaning the longest subsequence of words in the generated summary matches the original text perfectly.\n",
        "*  Summary:\n",
        "    - These near-perfect scores suggest that the generated summary is very close to a subset of the original text.\n",
        "    - However, this does not necessarily indicate a good summary quality because it might just mean that the generated summary is almost identical to portions of the original text rather than a concise representation of it.\n",
        "\n",
        "**Interpretation**\n",
        "  * Comparison with a Reference Summary `(calculate_rouge_scores1)`:\n",
        "  > - The ROUGE scores indicate the degree of overlap between the generated summary and a human reference summary.\n",
        "  > - Low `ROUGE-2` and `ROUGE-L` scores often suggest that the generated summary may lack coherence or detail compared to the reference. Improving the summary's quality (by adding more context, clarity, or coherence) would likely result in higher scores.\n",
        "  * Comparison with the Original Text `(calculate_rouge_scores2)`:\n",
        "  > - When comparing the generated summary directly to the original text, the ROUGE scores are almost perfect.\n",
        "  > - However, this is not always a good indicator of summarization quality. A good summary should be concise and capture the most important points rather than replicate large portions of the original text.\n",
        "\n",
        "**Conclusion**\n",
        "  * High ROUGE scores against the original text (as in `calculate_rouge_scores2`) suggest redundancy and that the generated summary is close to the original text.\n",
        "  * Moderate to Low ROUGE scores against a human reference summary (as in `calculate_rouge_scores1`) provide a more realistic evaluation of the quality of the summary. Improving `precision` and `recall` in `ROUGE-1`, `ROUGE-2`, and `ROUGE-L` will lead to a more concise, informative, and coherent summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OLoLHrhKCBuo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "CBXkAR8-I5fT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-yBYcnLI55E"
      },
      "source": [
        "##### **Incorporating BLEU and METEOR Scores for Evaluation**\n",
        "Now we will evaluate the quality of generated text summaries using two additional evaluation metrics: `BLEU (Bilingual Evaluation Understudy)` and `METEOR (Metric for Evaluation of Translation with Explicit ORdering)`. These metrics complement ROUGE by providing different perspectives on text similarity:\n",
        "\n",
        "- `BLEU Score`: Measures how close the generated summary is to the reference summary by comparing n-grams.\n",
        "- `METEOR Score`: Considers synonymy, stemming, and word order, providing a more nuanced evaluation of the generated summary's alignment with the reference summary.\n",
        "\n",
        "The functions provided will calculate `BLEU` and `METEOR` scores for a generated summary compared to a reference summary. Both scores will help us assess the generated summaries' fluency, informativeness, and relevance more comprehensively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "JKYEthQsDhm-"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "\n",
        "def calculate_bleu_score(generated_summary, reference_summary):\n",
        "    \"\"\"\n",
        "    Calculate BLEU score for a generated summary compared to a reference summary.\n",
        "\n",
        "    Args:\n",
        "        generated_summary (str): The generated summary to evaluate.\n",
        "        reference_summary (str): The reference summary for comparison.\n",
        "\n",
        "    Returns:\n",
        "        float: The BLEU score.\n",
        "    \"\"\"\n",
        "    # Tokenize the summaries\n",
        "    generated_tokens = word_tokenize(generated_summary)\n",
        "    reference_tokens = [word_tokenize(reference_summary)]\n",
        "\n",
        "    # Calculate BLEU score with smoothing\n",
        "    bleu_score = sentence_bleu(reference_tokens, generated_tokens, smoothing_function=SmoothingFunction().method1)\n",
        "\n",
        "    # Print the BLEU score\n",
        "    print(\"BLEU Score:\", bleu_score)\n",
        "\n",
        "    return bleu_score\n",
        "\n",
        "def calculate_meteor_score(generated_summary, reference_summary):\n",
        "    \"\"\"\n",
        "    Calculate METEOR score for a generated summary compared to a reference summary.\n",
        "\n",
        "    Args:\n",
        "        generated_summary (str): The generated summary to evaluate.\n",
        "        reference_summary (str): The reference summary for comparison.\n",
        "\n",
        "    Returns:\n",
        "        float: The METEOR score.\n",
        "    \"\"\"\n",
        "    # Tokenize the summaries\n",
        "    generated_tokens = word_tokenize(generated_summary)\n",
        "    reference_tokens = word_tokenize(reference_summary)\n",
        "\n",
        "    # Calculate METEOR score\n",
        "    meteor_score_value = meteor_score([' '.join(reference_tokens)], ' '.join(generated_tokens))\n",
        "\n",
        "    # Print the METEOR score\n",
        "    print(\"METEOR Score:\", meteor_score_value)\n",
        "\n",
        "    return meteor_score_value\n",
        "\n",
        "\n",
        "def calculate_meteor_score(generated_summary, reference_summary):\n",
        "    \"\"\"\n",
        "    Calculate METEOR score for a generated summary compared to a reference summary.\n",
        "\n",
        "    Args:\n",
        "        generated_summary (str): The generated summary to evaluate.\n",
        "        reference_summary (str): The reference summary for comparison.\n",
        "\n",
        "    Returns:\n",
        "        float: The METEOR score.\n",
        "    \"\"\"\n",
        "    # Tokenize the summaries\n",
        "    generated_tokens = word_tokenize(generated_summary)\n",
        "    reference_tokens = word_tokenize(reference_summary)\n",
        "\n",
        "    # Calculate METEOR score using tokenized inputs\n",
        "    meteor_score_value = meteor_score([reference_tokens], generated_tokens)  # Pass lists of tokens\n",
        "\n",
        "    # Print the METEOR score\n",
        "    print(\"METEOR Score:\", meteor_score_value)\n",
        "\n",
        "    return meteor_score_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-e6wEzzUEFoc",
        "outputId": "ff4b4ed4-12dd-46bf-8d5a-ee85ac3b5e15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU Score: 0.006436573612432792\n",
            "METEOR Score: 0.19305090744270662\n"
          ]
        }
      ],
      "source": [
        "# Calculate and display BLEU score\n",
        "bleu_score = calculate_bleu_score(summary, reference_summary)\n",
        "\n",
        "# Calculate and display METEOR score\n",
        "meteor_score_value = calculate_meteor_score(summary, reference_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "vVPQoE_fTDh9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-i0mXb8TEhh"
      },
      "source": [
        "###### **BLEU Score Explanation**\n",
        "* BLEU Score: 0.0064\n",
        "\n",
        "> - Range: BLEU scores range from 0 to 1, where 1 indicates a perfect match with the reference.\n",
        "> - Interpretation:\n",
        "    * A BLEU score of 0.0064 is extremely low, suggesting that the generated summary has very few words or phrases in common with the reference summary.\n",
        "    * This might indicate that the generated summary is either very different in content, style, or structure from the reference summary, or it may not be coherent or relevant to the reference text.\n",
        "\n",
        "###### **METEOR Score Explanation**\n",
        "* METEOR Score: 0.1931\n",
        "\n",
        "> - Range: METEOR scores also range from 0 to 1, where 1 represents a perfect match.\n",
        "> - Interpretation:\n",
        "    * A METEOR score of 0.1931 is relatively low but higher than the BLEU score, indicating some degree of overlap in terms of content, synonyms, and grammatical structure.\n",
        "    * METEOR takes into account partial matches and semantic similarities (like synonyms), so a score in this range often suggests that while the generated summary captures some parts of the meaning, it might not be a close or highly accurate representation of the reference summary.\n",
        "\n",
        "###### **Comparison and Analysis**\n",
        "* The BLEU score is very low, showing that the generated summary has minimal word-level overlap with the reference summary.\n",
        "  - This could be due to a different choice of words, phrases, or even poor coherence in the generated text.\n",
        "* The METEOR score is relatively higher compared to the BLEU score, suggesting that while the word-for-word match is poor, there is some level of semantic similarity or overlap.\n",
        "  - METEOR’s ability to account for synonyms and variations in word forms might explain why it has a higher score.\n",
        "\n",
        "**Possible Reasons for Low Scores**\n",
        "- Differences in Content: The generated summary might have a different focus or content than the reference summary.\n",
        "- Length of the Summaries: If the generated summary is too short or too long compared to the reference summary, it can affect both scores.\n",
        "- Quality of the Generated Summary: The generated summary might not be coherent, relevant, or informative enough compared to the reference summary.\n",
        "\n",
        "###### **Conclusion**\n",
        "In this case, both the BLEU and METEOR scores indicate that the generated summary is not closely aligned with the reference summary, although METEOR suggests there might be some semantic overlap. Improvements in the summarization algorithm, such as better handling of key content or phrase selection, may help enhance these scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Cq47NB4PI5GG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmETusbWWO1p"
      },
      "source": [
        "### **Conclusion and Takeaways:** Extractive Text Summarization\n",
        "- Extractive text summarization is a powerful technique in Natural Language Processing (NLP) that aims to condense a longer text into a shorter version while preserving its most important information.\n",
        "- The process involves selecting sentences or phrases directly from the original text that are deemed most relevant, based on various statistical and linguistic features.\n",
        "\n",
        "**Key Takeaways from the Process:**\n",
        "\n",
        "- Understanding Extractive Summarization:\n",
        "  * Extractive summarization involves selecting key sentences or phrases from the original text to form a concise summary.\n",
        "  * Unlike abstractive summarization, it does not generate new sentences or rephrase existing ones.\n",
        "  * Common features used for extraction include term frequency, sentence position, sentence length, named entities, and similarity with the title, among others.\n",
        "- Steps in Extractive Summarization:\n",
        "  * **Preprocessing:** Tokenization, stop-word removal, and other preprocessing techniques are applied to clean the text and prepare it for analysis.\n",
        "  * **Scoring Sentences:** Various algorithms (such as TF-IDF, cosine similarity, or graph-based approaches) are used to score each sentence's importance in the context of the document.\n",
        "  * **Selecting Sentences:** Based on the scoring mechanism, the top-ranked sentences are selected to form the summary.\n",
        "- Incorporating Advanced Features and Libraries:\n",
        "  * Improving the summarization model by incorporating additional features like sentence length, position, and named entities can significantly enhance the quality of the generated summary.\n",
        "  * Libraries such as `NLTK`, and `spaCy`, & (not used here) `Gensim`, and `Sumy` provide advanced methods and algorithms for extractive summarization, offering more flexibility and customization.\n",
        "- Evaluation of Summaries:\n",
        "  * Evaluation metrics like `ROUGE`, `BLEU`, and `METEOR` provide a quantitative measure of the quality of the summaries generated.\n",
        "  * ROUGE Scores (`ROUGE-1`, `ROUGE-2`, `ROUGE-L`) focus on `n-gram` overlap between the generated summary and reference summary, considering `precision`, `recall`, and `F-measure`.\n",
        "  * `BLEU` Score evaluates the `precision` of `n-gram` matches, while `METEOR` Score considers both `precision` and `recall`, and incorporates synonyms and stemming, offering a more semantic evaluation.\n",
        "- Challenges in Extractive Summarization:\n",
        "  * **Identifying Salient Information:** Determining which parts of the text are most relevant and important for summarization remains a challenge.\n",
        "  * **Generating Coherent Summaries:** While extractive summarization guarantees grammatical correctness (since it uses original sentences), ensuring logical flow and coherence remains a challenge.\n",
        "  * **Balancing Precision and Recall:** Striking the right balance between including all relevant information (recall) and avoiding irrelevant information (precision) is crucial.\n",
        "\n",
        "**Overall Conclusion**\n",
        "\n",
        "Extractive summarization is a well-established technique in NLP with practical applications in news summarization, document summarization, meeting summarization, and more. While effective, it has limitations in generating summaries that are truly concise and paraphrased. Nonetheless, by carefully selecting scoring mechanisms, incorporating various NLP features, and leveraging evaluation metrics, we can generate effective extractive summaries that are useful for many real-world applications. Future advancements may involve hybrid approaches that combine the precision of extractive methods with the fluency and creativity of abstractive methods, creating even more powerful summarization systems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "WNtVja6Mcmum"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 ('capstone')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "6c67f8adbebdea49f93c63d2a2dcd5cb4fed3a722249876be11f90d0fd0a91fc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
