{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kkrusere/NLP-Text-Summarization/blob/main/text_summarization_using_abstractive_method.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujwb2FAFcecN"
      },
      "source": [
        "## <center>**NLP Text Summarization** <center><em>\n",
        "**<center>Abstractive Summarization</center>**\n",
        "\n",
        "Text summarization refers to the technique of shortening long pieces of text, with the intention of creating a coherent and fluent summary having only the main points outlined in the document. Basically, the process of creating shorter text without removing the semantic structure of text.\n",
        "</em></center>\n",
        "<br>\n",
        "<center><img src=\"https://github.com/kkrusere/NLP-Text-Summarization/blob/main/assets/mchinelearning_text_sum.png?raw=1\" width=600/></center>\n",
        "\n",
        "***Project Contributors:*** Kuzi Rusere<br>\n",
        "**MVP streamlit App URL:** N/A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jaVdkFXlcecO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f23e7442-9bba-45fb-89c8-ce2795f5ceff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import spacy\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b4wNJqHcLm0"
      },
      "source": [
        "**Abstractive summarization** involves understanding the core ideas of the text and then creating a new, condensed version that expresses those ideas, potentially using different words and phrasing. Unlike extractive summarization, which relies on selecting sentences or phrases from the text, abstractive summarization generates summaries that may not directly reuse sentences from the original text but instead create a human-like paraphrased version of the summary. It can be more complex because it requires the ability to truly understand the text and create meaningful new text that represents it."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QXk-J9SvdAVs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQJ5m19AcecP"
      },
      "source": [
        "For our example text, we are going use this brief explainer of the history of Chaos theory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yXjD8jTgcecP"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "In 1961, a meteorologist by the name of Edward Lorenz made a profound discovery. Lorenz was utilising the new-found power of computers in an attempt to more accurately predict the weather. He created a mathematical model which, when supplied with a set of numbers representing the current weather, could predict the weather a few minutes in advance.\n",
        "Once this computer program was up and running, Lorenz could produce long-term forecasts by feeding the predicted weather back into the computer over and over again, with each run forecasting further into the future.Accurate minute-by-minute forecasts added up into days, and then weeks.\n",
        "One day, Lorenz decided to rerun one of his forecasts. In the interests of saving time he decided not to start from scratch; instead he took the computer’s prediction from halfway through the first run and used that as the starting point.\n",
        "After a well-earned coffee break, he returned to discover something unexpected. Although the computer’s new predictions started out the same as before, the two sets of predictions soon began diverging drastically. What had gone wrong?\n",
        "Lorenz soon realised that while the computer was printing out the predictions to three decimal places, it was actually crunching the numbers internally using six decimal places.\n",
        "So while Lorenz had started the second run with the number 0.506, the original run had used the number 0.506127.\n",
        "A difference of one part in a thousand: the same sort of difference that a flap of a butterfly’s wing might make to the breeze on your face. The starting weather conditions had been virtually identical. The two predictions were anything but.\n",
        "Lorenz had found the seeds of chaos. In systems that behave nicely - without chaotic effects - small differences only produce small effects. In this case, Lorenz’s equations were causing errors to steadily grow over time.\n",
        "This meant that tiny errors in the measurement of the current weather would not stay tiny, but relentlessly increased in size each time they were fed back into the computer until they had completely swamped the predictions.\n",
        "Lorenz famously illustrated this effect with the analogy of a butterfly flapping its wings and thereby causing the formation of a hurricane half a world away.\n",
        "A nice way to see this “butterfly effect” for yourself is with a game of pool or billiards. No matter how consistent you are with the first shot (the break), the smallest of differences in the speed and angle with which you strike the white ball will cause the pack of billiards to scatter in wildly different directions every time.\n",
        "The smallest of differences are producing large effects - the hallmark of a chaotic system.\n",
        "It is worth noting that the laws of physics that determine how the billiard balls move are precise and unambiguous: they allow no room for randomness.\n",
        "What at first glance appears to be random behaviour is completely deterministic - it only seems random because imperceptible changes are making all the difference.\n",
        "The rate at which these tiny differences stack up provides each chaotic system with a prediction horizon - a length of time beyond which we can no longer accurately forecast its behaviour.\n",
        "In the case of the weather, the prediction horizon is nowadays about one week (thanks to ever-improving measuring instruments and models).\n",
        "Some 50 years ago it was 18 hours. Two weeks is believed to be the limit we could ever achieve however much better computers and software get.\n",
        "Surprisingly, the solar system is a chaotic system too - with a prediction horizon of a hundred million years. It was the first chaotic system to be discovered, long before there was a Chaos Theory.\n",
        "In 1887, the French mathematician Henri Poincaré showed that while Newton’s theory of gravity could perfectly predict how two planetary bodies would orbit under their mutual attraction, adding a third body to the mix rendered the equations unsolvable.\n",
        "The best we can do for three bodies is to predict their movements moment by moment, and feed those predictions back into our equations …\n",
        "Though the dance of the planets has a lengthy prediction horizon, the effects of chaos cannot be ignored, for the intricate interplay of gravitation tugs among the planets has a large influence on the trajectories of the asteroids.\n",
        "Keeping an eye on the asteroids is difficult but worthwhile, since such chaotic effects may one day fling an unwelcome surprise our way.\n",
        "On the flip side, they can also divert external surprises such as steering comets away from a potential collision with Earth.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Abstractive Summarization** using NLTK, spaCy, Gensim, and Sumy\n",
        "\n",
        "Abstractive summarization using traditional NLP libraries like NLTK, spaCy, Gensim, and Sumy can be more challenging since these libraries are more commonly used for extractive summarization. However, we can create a basic approach to mimic abstractive summarization by combining various techniques."
      ],
      "metadata": {
        "id": "ukWN8Jzcr58Q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7eWe5WL7swYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Preprocessing the Text\n",
        "- Before we start with the summarization, we need to preprocess the text to clean and prepare it."
      ],
      "metadata": {
        "id": "5OY2M4uZsv2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Tokenize sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and punctuation\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    processed_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence)\n",
        "        filtered_words = [word.lower() for word in words if word.lower() not in stop_words and word not in string.punctuation]\n",
        "        processed_sentences.append(' '.join(filtered_words))\n",
        "\n",
        "    return processed_sentences\n"
      ],
      "metadata": {
        "id": "-HyXi9iAr5n_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_text = preprocess_text(text)\n",
        "preprocessed_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nU-Q9yHOtFUl",
        "outputId": "9f6c5960-b4cb-4a83-820a-8229056f4d6c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1961 meteorologist name edward lorenz made profound discovery',\n",
              " 'lorenz utilising new-found power computers attempt accurately predict weather',\n",
              " 'created mathematical model supplied set numbers representing current weather could predict weather minutes advance',\n",
              " 'computer program running lorenz could produce long-term forecasts feeding predicted weather back computer run forecasting future.accurate minute-by-minute forecasts added days weeks',\n",
              " 'one day lorenz decided rerun one forecasts',\n",
              " 'interests saving time decided start scratch instead took computer ’ prediction halfway first run used starting point',\n",
              " 'well-earned coffee break returned discover something unexpected',\n",
              " 'although computer ’ new predictions started two sets predictions soon began diverging drastically',\n",
              " 'gone wrong',\n",
              " 'lorenz soon realised computer printing predictions three decimal places actually crunching numbers internally using six decimal places',\n",
              " 'lorenz started second run number 0.506 original run used number 0.506127',\n",
              " 'difference one part thousand sort difference flap butterfly ’ wing might make breeze face',\n",
              " 'starting weather conditions virtually identical',\n",
              " 'two predictions anything',\n",
              " 'lorenz found seeds chaos',\n",
              " 'systems behave nicely without chaotic effects small differences produce small effects',\n",
              " 'case lorenz ’ equations causing errors steadily grow time',\n",
              " 'meant tiny errors measurement current weather would stay tiny relentlessly increased size time fed back computer completely swamped predictions',\n",
              " 'lorenz famously illustrated effect analogy butterfly flapping wings thereby causing formation hurricane half world away',\n",
              " 'nice way see “ butterfly effect ” game pool billiards',\n",
              " 'matter consistent first shot break smallest differences speed angle strike white ball cause pack billiards scatter wildly different directions every time',\n",
              " 'smallest differences producing large effects hallmark chaotic system',\n",
              " 'worth noting laws physics determine billiard balls move precise unambiguous allow room randomness',\n",
              " 'first glance appears random behaviour completely deterministic seems random imperceptible changes making difference',\n",
              " 'rate tiny differences stack provides chaotic system prediction horizon length time beyond longer accurately forecast behaviour',\n",
              " 'case weather prediction horizon nowadays one week thanks ever-improving measuring instruments models',\n",
              " '50 years ago 18 hours',\n",
              " 'two weeks believed limit could ever achieve however much better computers software get',\n",
              " 'surprisingly solar system chaotic system prediction horizon hundred million years',\n",
              " 'first chaotic system discovered long chaos theory',\n",
              " '1887 french mathematician henri poincaré showed newton ’ theory gravity could perfectly predict two planetary bodies would orbit mutual attraction adding third body mix rendered equations unsolvable',\n",
              " 'best three bodies predict movements moment moment feed predictions back equations … though dance planets lengthy prediction horizon effects chaos ignored intricate interplay gravitation tugs among planets large influence trajectories asteroids',\n",
              " 'keeping eye asteroids difficult worthwhile since chaotic effects may one day fling unwelcome surprise way',\n",
              " 'flip side also divert external surprises steering comets away potential collision earth']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-ONbkYEetMfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Extract Keywords and Key Phrases\n",
        "- To create an abstractive summary, we need to identify key phrases and concepts from the text."
      ],
      "metadata": {
        "id": "LLpR7l9stONj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def extract_keywords(text, num_keywords=10):\n",
        "    doc = nlp(text)\n",
        "    # Extract noun chunks (key phrases)\n",
        "    keywords = [chunk.text for chunk in doc.noun_chunks]\n",
        "    # Get the most common keywords\n",
        "    common_keywords = Counter(keywords).most_common(num_keywords)\n",
        "    return common_keywords"
      ],
      "metadata": {
        "id": "UeYYZMQ4tOrv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = extract_keywords(text)\n",
        "keywords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mvtk2BotPyB",
        "outputId": "963064a7-6841-4f46-a7c2-1707fcd4e944"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Lorenz', 7),\n",
              " ('which', 4),\n",
              " ('they', 4),\n",
              " ('the weather', 3),\n",
              " ('the computer', 3),\n",
              " ('he', 3),\n",
              " ('it', 3),\n",
              " ('that', 3),\n",
              " ('we', 3),\n",
              " ('the current weather', 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nJohdLFrtPva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Generate Sentence Embeddings (Using Gensim)\n",
        "- We can use Gensim to create sentence embeddings, which will help in understanding the context and semantic similarity between sentences."
      ],
      "metadata": {
        "id": "lV8Dgg0lwtRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "def generate_sentence_embeddings(sentences):\n",
        "    # Tokenize sentences\n",
        "    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
        "\n",
        "    # Train Word2Vec model\n",
        "    model = Word2Vec(tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "    # Generate sentence embeddings by averaging word vectors\n",
        "    sentence_embeddings = []\n",
        "    for sentence in tokenized_sentences:\n",
        "        if len(sentence) > 0:\n",
        "            sentence_embedding = sum([model.wv[word] for word in sentence if word in model.wv]) / len(sentence)\n",
        "            sentence_embeddings.append(sentence_embedding)\n",
        "        else:\n",
        "            sentence_embeddings.append(None)\n",
        "\n",
        "    return sentence_embeddings"
      ],
      "metadata": {
        "id": "wK57deUBtPsj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_embeddings = generate_sentence_embeddings(preprocessed_text)\n",
        "sentence_embeddings"
      ],
      "metadata": {
        "id": "EsUb7Ji7tPp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u-RxlfTqtPnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Rank Sentences Based on Keywords and Similarity\n",
        "- We rank sentences by their relevance to the extracted keywords and the similarity of their embeddings to one another."
      ],
      "metadata": {
        "id": "PLokYam9xFFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def rank_sentences(sentences, keywords, sentence_embeddings):\n",
        "    # Rank based on keyword occurrence\n",
        "    keyword_sentences = [(sentence, sum(sentence.count(keyword[0]) for keyword in keywords)) for sentence in sentences]\n",
        "\n",
        "    # Rank based on similarity (optional, more for extractive purposes)\n",
        "    if sentence_embeddings:\n",
        "        similarity_matrix = cosine_similarity(sentence_embeddings)\n",
        "        similarity_scores = similarity_matrix.sum(axis=1)\n",
        "        combined_ranking = [(sentence, keyword_score + similarity_score) for (sentence, keyword_score), similarity_score in zip(keyword_sentences, similarity_scores)]\n",
        "    else:\n",
        "        combined_ranking = keyword_sentences\n",
        "\n",
        "    # Sort sentences by combined score\n",
        "    ranked_sentences = sorted(combined_ranking, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return ranked_sentences\n",
        "\n"
      ],
      "metadata": {
        "id": "pxPN0k7atPiZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ranked_sentences = rank_sentences(preprocessed_text, keywords, sentence_embeddings)\n",
        "ranked_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brJSVEQktPfB",
        "outputId": "33b18c73-ba41-4128-9c79-a00415ca93e1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('1887 french mathematician henri poincaré showed newton ’ theory gravity could perfectly predict two planetary bodies would orbit mutual attraction adding third body mix rendered equations unsolvable',\n",
              "  9.23914623260498),\n",
              " ('created mathematical model supplied set numbers representing current weather could predict weather minutes advance',\n",
              "  7.527207374572754),\n",
              " ('lorenz utilising new-found power computers attempt accurately predict weather',\n",
              "  6.4801695346832275),\n",
              " ('case weather prediction horizon nowadays one week thanks ever-improving measuring instruments models',\n",
              "  6.367793560028076),\n",
              " ('computer program running lorenz could produce long-term forecasts feeding predicted weather back computer run forecasting future.accurate minute-by-minute forecasts added days weeks',\n",
              "  6.196478843688965),\n",
              " ('two weeks believed limit could ever achieve however much better computers software get',\n",
              "  6.0695788860321045),\n",
              " ('meant tiny errors measurement current weather would stay tiny relentlessly increased size time fed back computer completely swamped predictions',\n",
              "  5.471256732940674),\n",
              " ('one day lorenz decided rerun one forecasts', 5.428025722503662),\n",
              " ('starting weather conditions virtually identical', 5.008862733840942),\n",
              " ('best three bodies predict movements moment moment feed predictions back equations … though dance planets lengthy prediction horizon effects chaos ignored intricate interplay gravitation tugs among planets large influence trajectories asteroids',\n",
              "  4.7690863609313965),\n",
              " ('case lorenz ’ equations causing errors steadily grow time',\n",
              "  4.722980499267578),\n",
              " ('interests saving time decided start scratch instead took computer ’ prediction halfway first run used starting point',\n",
              "  4.599942684173584),\n",
              " ('although computer ’ new predictions started two sets predictions soon began diverging drastically',\n",
              "  4.498040199279785),\n",
              " ('first chaotic system discovered long chaos theory', 4.46635365486145),\n",
              " ('matter consistent first shot break smallest differences speed angle strike white ball cause pack billiards scatter wildly different directions every time',\n",
              "  4.452732801437378),\n",
              " ('systems behave nicely without chaotic effects small differences produce small effects',\n",
              "  4.442891359329224),\n",
              " ('smallest differences producing large effects hallmark chaotic system',\n",
              "  4.277419090270996),\n",
              " ('keeping eye asteroids difficult worthwhile since chaotic effects may one day fling unwelcome surprise way',\n",
              "  4.195662498474121),\n",
              " ('difference one part thousand sort difference flap butterfly ’ wing might make breeze face',\n",
              "  3.5750999450683594),\n",
              " ('lorenz found seeds chaos', 3.4611141681671143),\n",
              " ('rate tiny differences stack provides chaotic system prediction horizon length time beyond longer accurately forecast behaviour',\n",
              "  3.079813003540039),\n",
              " ('first glance appears random behaviour completely deterministic seems random imperceptible changes making difference',\n",
              "  3.011192560195923),\n",
              " ('lorenz famously illustrated effect analogy butterfly flapping wings thereby causing formation hurricane half world away',\n",
              "  2.6307088136672974),\n",
              " ('flip side also divert external surprises steering comets away potential collision earth',\n",
              "  2.596940040588379),\n",
              " ('lorenz started second run number 0.506 original run used number 0.506127',\n",
              "  2.135995388031006),\n",
              " ('two predictions anything', 2.084109306335449),\n",
              " ('surprisingly solar system chaotic system prediction horizon hundred million years',\n",
              "  2.0755367279052734),\n",
              " ('well-earned coffee break returned discover something unexpected',\n",
              "  1.763897955417633),\n",
              " ('1961 meteorologist name edward lorenz made profound discovery',\n",
              "  1.5349479913711548),\n",
              " ('worth noting laws physics determine billiard balls move precise unambiguous allow room randomness',\n",
              "  1.2191485166549683),\n",
              " ('gone wrong', 1.0588966608047485),\n",
              " ('nice way see “ butterfly effect ” game pool billiards', 0.9880290031433105),\n",
              " ('lorenz soon realised computer printing predictions three decimal places actually crunching numbers internally using six decimal places',\n",
              "  0.7536181211471558),\n",
              " ('50 years ago 18 hours', -0.6785022616386414)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sZKq04XvtPX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Generate Abstractive Summary\n",
        "- Finally, we can create an abstractive summary by paraphrasing and rephrasing the top-ranked sentences."
      ],
      "metadata": {
        "id": "8YqqI3MGyQQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def paraphrase_sentence(sentence):\n",
        "    words = word_tokenize(sentence)\n",
        "    random.shuffle(words)\n",
        "    paraphrased_sentence = ' '.join(words)\n",
        "    return paraphrased_sentence\n",
        "\n",
        "def generate_abstractive_summary(ranked_sentences, num_sentences=3):\n",
        "    top_sentences = [sentence[0] for sentence in ranked_sentences[:num_sentences]]\n",
        "    paraphrased_sentences = [paraphrase_sentence(sentence) for sentence in top_sentences]\n",
        "    summary = ' '.join(paraphrased_sentences)\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "DCDepHm5tPHW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = generate_abstractive_summary(ranked_sentences)\n",
        "summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "Fptka3xNylgp",
        "outputId": "18ea58f4-9bee-4ffb-ac99-dac925c0aa5e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'equations adding predict would bodies mathematician unsolvable perfectly theory attraction orbit 1887 henri mutual rendered planetary two french body gravity mix newton could poincaré third ’ showed supplied minutes representing predict set created model numbers mathematical weather weather current could advance lorenz attempt accurately weather utilising predict power computers new-found'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Summary=\n",
        "        equations adding predict would bodies mathematician unsolvable perfectly theory attraction orbit 1887 henri mutual rendered planetary two\n",
        "        french body gravity mix newton could poincaré third ’ showed supplied minutes representing predict set created model numbers mathematical\n",
        "        weather weather current could advance lorenz attempt accurately weather utilising predict power computers new-found\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "BSfZqYZf0IYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The generated summary is not a good summary of the input text (as expected).\n",
        "\n",
        "- What the summary is focusing on:\n",
        "> - The summary primarily highlights the concept of chaos theory and its impact on weather prediction.\n",
        "> - It mentions Edward Lorenz's work with computer models and the butterfly effect.\n",
        "> - It also briefly touches upon the solar system as a chaotic system.\n",
        "\n",
        "* What the summary is missing:\n",
        "> - The detailed explanation of Lorenz's experiment:\n",
        "  > > * The text provides a step-by-step account of how Lorenz discovered chaos, including the specific details of his computer model, the rounding error, and the resulting divergence in predictions.\n",
        "  > > *This is crucial to understanding the core concept of chaos theory.\n",
        "> - The connection between chaos and determinism:\n",
        "  > > * The text emphasizes that chaotic systems, while seemingly random, are actually governed by deterministic laws.\n",
        "    > > * The summary completely omits this important point.\n",
        "> - The prediction horizon and its implications:\n",
        "  > > * The text explains the concept of the prediction horizon and its relevance to weather forecasting and the solar system.\n",
        "  > > * The summary mentions the prediction horizon in relation to the solar system but fails to connect it to the broader theme of chaos theory's limitations on predictability.\n",
        "> - The role of chaos in the solar system and asteroid trajectories:\n",
        "  > > * The text discusses how chaos affects the solar system, particularly the movement of asteroids.\n",
        "  > > * The summary briefly mentions the solar system as chaotic but neglects the specific implications for asteroids and potential collisions with Earth.\n",
        "\n",
        "- What it lacks:\n",
        "> - Clarity and coherence:\n",
        "  > > * The summary feels disjointed and lacks a clear flow of ideas.\n",
        "  > > * The sentences are poorly connected, making it difficult to follow the overall narrative.\n",
        "> - Comprehensiveness:\n",
        "  > > * The summary fails to capture the full scope of the text, omitting key concepts and examples that are crucial for understanding chaos theory.\n",
        "> - Accuracy:\n",
        "  > > * The summary oversimplifies some aspects of chaos theory, potentially leading to misunderstandings.\n",
        "\n",
        "In essence, the summary provides a very superficial overview of chaos theory, focusing mainly on its impact on weather prediction. It misses out on the rich details, explanations, and connections that make the original text informative and engaging."
      ],
      "metadata": {
        "id": "cHehu9_t1FA-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CbAGJ8iE1EJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6QpDGIH-zC3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets incooporate some Evaluation:\n",
        "\n",
        "To evaluate the quality of the summaries generated by our extractive summarization algorithm, we can use several evaluation metrics. The most widely used evaluation metric for summarization tasks is ROUGE (Recall-Oriented Understudy for Gisting Evaluation). Other evaluation metrics include BLEU (Bilingual Evaluation Understudy) and METEOR (Metric for Evaluation of Translation with Explicit ORdering).\n",
        "\n",
        "Evaluation Metrics for Summarization\n",
        "- ROUGE (Recall-Oriented Understudy for Gisting Evaluation):\n",
        "    * ROUGE-1: Measures the overlap of unigrams (single words) between the generated summary and a reference summary.\n",
        "    * ROUGE-2: Measures the overlap of bigrams (two consecutive words).\n",
        "    * ROUGE-L: Measures the longest common subsequence (LCS) between the generated and reference summaries, capturing the in-sequence overlap.\n",
        "- BLEU (Bilingual Evaluation Understudy):\n",
        "    * Primarily used for machine translation but can be adapted for summarization.\n",
        "    * Measures n-gram precision of a generated text concerning one or more reference texts.\n",
        "- METEOR (Metric for Evaluation of Translation with Explicit ORdering):\n",
        "    * Designed to improve BLEU by addressing problems like synonymy and stemming.\n",
        "    * It considers unigram matches between generated and reference summaries, applying stemming and synonymy matching.\n"
      ],
      "metadata": {
        "id": "aECv-5mOzDPL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF6iu7wpcic0"
      },
      "source": [
        "#### **Conclusion**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WNtVja6Mcmum"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 ('capstone')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "6c67f8adbebdea49f93c63d2a2dcd5cb4fed3a722249876be11f90d0fd0a91fc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}