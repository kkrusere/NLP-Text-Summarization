{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujwb2FAFcecN"
      },
      "source": [
        "## <center>**NLP Text Summarization Using NLTK** </center>\n",
        "**<center>Extractive Summarization</center>**\n",
        "<center><em>\n",
        "Text summarization refers to the technique of shortening long pieces of text, with the intention of creating a coherent and fluent summary having only the main points outlined in the document. Basically, the process of creating shorter text without removing the semantic structure of text. \n",
        "</em></center>\n",
        "<br>\n",
        "<center><img src=\"https://github.com/kkrusere/NLP-Text-Summarization/blob/main/assets/mchinelearning_text_sum.png?raw=1\" width=600/></center>\n",
        "\n",
        "***Project Contributors:*** Kuzi Rusere<br>\n",
        "**MVP streamlit App URL:** N/A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Extractive summarization** focuses on picking out the most essential phrases and sentences directly from the original text and putting them together to create a shorter version. The original text is not changed in any way. The strength of this method is its accuracy; because it uses the original words, it's very reliable for keeping facts straight."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQJ5m19AcecP"
      },
      "source": [
        "For our example text, we are going use this brief explainer of the history of Chaos theory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yXjD8jTgcecP"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "In 1961, a meteorologist by the name of Edward Lorenz made a profound discovery. Lorenz was utilising the new-found power of computers in an attempt to more accurately predict the weather. He created a mathematical model which, when supplied with a set of numbers representing the current weather, could predict the weather a few minutes in advance.\n",
        "Once this computer program was up and running, Lorenz could produce long-term forecasts by feeding the predicted weather back into the computer over and over again, with each run forecasting further into the future.Accurate minute-by-minute forecasts added up into days, and then weeks.\n",
        "One day, Lorenz decided to rerun one of his forecasts. In the interests of saving time he decided not to start from scratch; instead he took the computer’s prediction from halfway through the first run and used that as the starting point.\n",
        "After a well-earned coffee break, he returned to discover something unexpected. Although the computer’s new predictions started out the same as before, the two sets of predictions soon began diverging drastically. What had gone wrong?\n",
        "Lorenz soon realised that while the computer was printing out the predictions to three decimal places, it was actually crunching the numbers internally using six decimal places.\n",
        "So while Lorenz had started the second run with the number 0.506, the original run had used the number 0.506127.\n",
        "A difference of one part in a thousand: the same sort of difference that a flap of a butterfly’s wing might make to the breeze on your face. The starting weather conditions had been virtually identical. The two predictions were anything but.\n",
        "Lorenz had found the seeds of chaos. In systems that behave nicely - without chaotic effects - small differences only produce small effects. In this case, Lorenz’s equations were causing errors to steadily grow over time.\n",
        "This meant that tiny errors in the measurement of the current weather would not stay tiny, but relentlessly increased in size each time they were fed back into the computer until they had completely swamped the predictions.\n",
        "Lorenz famously illustrated this effect with the analogy of a butterfly flapping its wings and thereby causing the formation of a hurricane half a world away.\n",
        "A nice way to see this “butterfly effect” for yourself is with a game of pool or billiards. No matter how consistent you are with the first shot (the break), the smallest of differences in the speed and angle with which you strike the white ball will cause the pack of billiards to scatter in wildly different directions every time.\n",
        "The smallest of differences are producing large effects - the hallmark of a chaotic system.\n",
        "It is worth noting that the laws of physics that determine how the billiard balls move are precise and unambiguous: they allow no room for randomness.\n",
        "What at first glance appears to be random behaviour is completely deterministic - it only seems random because imperceptible changes are making all the difference.\n",
        "The rate at which these tiny differences stack up provides each chaotic system with a prediction horizon - a length of time beyond which we can no longer accurately forecast its behaviour.\n",
        "In the case of the weather, the prediction horizon is nowadays about one week (thanks to ever-improving measuring instruments and models).\n",
        "Some 50 years ago it was 18 hours. Two weeks is believed to be the limit we could ever achieve however much better computers and software get.\n",
        "Surprisingly, the solar system is a chaotic system too - with a prediction horizon of a hundred million years. It was the first chaotic system to be discovered, long before there was a Chaos Theory.\n",
        "In 1887, the French mathematician Henri Poincaré showed that while Newton’s theory of gravity could perfectly predict how two planetary bodies would orbit under their mutual attraction, adding a third body to the mix rendered the equations unsolvable.\n",
        "The best we can do for three bodies is to predict their movements moment by moment, and feed those predictions back into our equations …\n",
        "Though the dance of the planets has a lengthy prediction horizon, the effects of chaos cannot be ignored, for the intricate interplay of gravitation tugs among the planets has a large influence on the trajectories of the asteroids.\n",
        "Keeping an eye on the asteroids is difficult but worthwhile, since such chaotic effects may one day fling an unwelcome surprise our way.\n",
        "On the flip side, they can also divert external surprises such as steering comets away from a potential collision with Earth.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Key Steps in Extractive Summarization:\n",
        "\n",
        "1. **Text Preprocessing:** Cleaning and preprocessing the text (e.g., remove stopwords, punctuation, and lowercasing).\n",
        "2. **Sentence Tokenization:** Splitting the text into individual sentences.\n",
        "3. **Word Tokenization and Normalization:** Tokenizing sentences into words and apply normalization techniques like stemming or lemmatization.\n",
        "4. **Scoring Sentences:** Assigning a score to each sentence based on different features (e.g., word frequency, sentence position, presence of keywords).\n",
        "5. **Rank and Sentence Selection:** Ranking sentences based on their scores and select the top-ranked ones for the summary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**First What is NLTK in Natural Language Processing (NLP)?**\n",
        "\n",
        "NLTK, or Natural Language Toolkit, is a powerful and widely-used Python library for working with human language data in the field of Natural Language Processing (NLP).\n",
        "\n",
        "\n",
        "* Core Functionality: NLTK provides a comprehensive suite of tools and resources for various NLP tasks including:   \n",
        "\n",
        "    - Text preprocessing: Tokenization, stemming, lemmatization, stop word removal, part-of-speech tagging   \n",
        "    - Corpus access: Interfaces to numerous text corpora and lexical resources   \n",
        "    - Text analysis: Concordance, frequency distribution, collocations, and more   \n",
        "    - Machine learning: Classification, named entity recognition, and other NLP applications   \n",
        "\n",
        "* Benefits:\n",
        "\n",
        "    - Ease of use: NLTK's user-friendly interface and extensive documentation make it accessible for both beginners and experienced NLP practitioners.   \n",
        "    - Versatility: The library supports a wide range of NLP tasks and techniques, enabling flexibility in project development.   \n",
        "    - Community and Resources: NLTK boasts a large and active community, providing ample support and resources for learning and troubleshooting.   \n",
        "\n",
        "* Applications: NLTK is employed in various NLP projects, including:   \n",
        "\n",
        "    - Sentiment analysis   \n",
        "    - Chatbots and conversational agents   \n",
        "    - Text summarization   \n",
        "    - Machine translation   \n",
        "    - Information extraction \n",
        "  \n",
        "\n",
        "In essence, NLTK is a foundational tool in the NLP landscape, offering a rich set of functionalities and resources to facilitate the development of diverse natural language processing applications.   \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "import string\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "1. Preprocessing the Text:\n",
        "    - The text is tokenized into sentences and words.\n",
        "    - Stop_words (common words like \"the,\" \"is,\" etc.) and punctuation are removed to focus on meaningful words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_text(text, stop_words = stop_words):\n",
        "    # Tokenize text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Tokenize sentences into words\n",
        "    words = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove stopwords and punctuation\n",
        "    filtered_words = [word for word in words if word not in stop_words and word not in string.punctuation]\n",
        "\n",
        "    return sentences, filtered_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Word Frequency Calculation:\n",
        "- A frequency distribution of the filtered words is calculated to determine the importance of each word.\n",
        "- Frequencies are normalized to bring them to a common scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_word_frequency(filtered_words):\n",
        "    # Frequency distribution of words\n",
        "    freq_dist = FreqDist(filtered_words)\n",
        "    \n",
        "    # Normalize frequencies\n",
        "    max_freq = max(freq_dist.values())\n",
        "    for word in freq_dist.keys():\n",
        "        freq_dist[word] = (freq_dist[word] / max_freq)\n",
        "    \n",
        "    return freq_dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Sentence Scoring:\n",
        "- Each sentence is scored based on the sum of the frequencies of the words it contains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def score_sentences(sentences, freq_dist):\n",
        "    # Score each sentence based on the frequency of words it contains\n",
        "    sentence_scores = {}\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence.lower())\n",
        "        score = 0\n",
        "        for word in words:\n",
        "            if word in freq_dist:\n",
        "                score += freq_dist[word]\n",
        "        sentence_scores[sentence] = score\n",
        "\n",
        "    return sentence_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Generating the Summary:\n",
        "- Sentences are sorted based on their scores, and the top sentences are selected to form the summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_summary(sentence_scores, num_sentences=3):\n",
        "    # Sort sentences by their scores in descending order\n",
        "    sorted_sentences = sorted(sentence_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    # Select the top 'num_sentences' sentences\n",
        "    summary_sentences = [sentence[0] for sentence in sorted_sentences[:num_sentences]]\n",
        "    \n",
        "    # Join the selected sentences to form the summary\n",
        "    summary = ' '.join(summary_sentences)\n",
        "    \n",
        "    return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary:\n",
            "\n",
            "The best we can do for three bodies is to predict their movements moment by moment, and feed those predictions back into our equations …\n",
            "Though the dance of the planets has a lengthy prediction horizon, the effects of chaos cannot be ignored, for the intricate interplay of gravitation tugs among the planets has a large influence on the trajectories of the asteroids. Once this computer program was up and running, Lorenz could produce long-term forecasts by feeding the predicted weather back into the computer over and over again, with each run forecasting further into the future.Accurate minute-by-minute forecasts added up into days, and then weeks. This meant that tiny errors in the measurement of the current weather would not stay tiny, but relentlessly increased in size each time they were fed back into the computer until they had completely swamped the predictions.\n"
          ]
        }
      ],
      "source": [
        "# Execute the extractive summarization steps\n",
        "sentences, filtered_words = preprocess_text(text)\n",
        "freq_dist = compute_word_frequency(filtered_words)\n",
        "sentence_scores = score_sentences(sentences, freq_dist)\n",
        "summary = generate_summary(sentence_scores)\n",
        "\n",
        "# Output the summary\n",
        "print(\"Summary:\\n\")\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \"\"\"Summary:\n",
        "\n",
        "# The best we can do for three bodies is to predict their movements moment by moment, and feed those predictions back into our equations …\n",
        "# Though the dance of the planets has a lengthy prediction horizon, the effects of chaos cannot be ignored, for the intricate interplay of \n",
        "# gravitation tugs among the planets has a large influence on the trajectories of the asteroids. Once this computer program was up and running, \n",
        "# Lorenz could produce long-term forecasts by feeding the predicted weather back into the computer over and over again, with each run forecasting \n",
        "# further into the future.Accurate minute-by-minute forecasts added up into days, and then weeks. This meant that tiny errors in the measurement \n",
        "# of the current weather would not stay tiny, but relentlessly increased in size each time they were fed back into the computer until they had \n",
        "# completely swamped the predictions.\n",
        "# \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output summary is not a good summary of the original text.\n",
        "\n",
        "Here's why:\n",
        "\n",
        "- **Focus on a Specific Example:** The summary primarily focuses on the specific example of Lorenz's weather prediction model. While this is a crucial part of the text, it neglects the broader concept of chaos theory and its implications in other systems.\n",
        "- **The summary fails to mention key ideas like:**\n",
        "    - **Deterministic Chaos:** The idea that seemingly random behavior can arise from deterministic systems due to sensitivity to initial conditions.\n",
        "    - **Prediction Horizon:** The time limit beyond which accurate predictions become impossible in a chaotic system.\n",
        "    - **Chaos in the Solar System:** The chaotic nature of the solar system and its implications for asteroid trajectories.\n",
        "- **Lacks Context and Flow:** The summary feels disjointed, jumping straight into Lorenz's experiment without providing sufficient context about chaos theory and its significance.\n",
        "\n",
        "The summary provides a narrow view by focusing on a single example and neglecting the broader concepts and implications of chaos theory explored in the original text. There is need to improve the summarization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Let's get into how we can Improve the Scoring Mechanism\n",
        "- To improve the extractive summarization approach, we can incorporate additional features to score sentences more accurately. Here are some ways to enhance the scoring mechanism:\n",
        "\n",
        "Additional Features for Scoring:\n",
        "\n",
        "1. Sentence Length:\n",
        "    - Avoiding very short or too long sentences, which may be less informative or difficult to understand.\n",
        "    - We can add a penalty or a boost to the sentence score based on its length.\n",
        "2. Sentence Position:\n",
        "    - In many texts (e.g., news articles), important information is often found in the first few sentences.\n",
        "    - We can give a higher score to sentences appearing earlier in the text.\n",
        "3. Named Entity Recognition (NER):\n",
        "    - Sentences containing named entities (like people, places, organizations) may carry more important information.\n",
        "    - We can boost the score of sentences that contain named entities.\n",
        "4. Thematic Words:\n",
        "    - Words that appear frequently throughout the text are often central to the main idea.\n",
        "    - We can further emphasize the sentences containing these thematic words.\n",
        "\n",
        "\n",
        "\n",
        "Let's incorporate these features into the revised version of extractive summarization code using NLTK and spaCy\n",
        "\n",
        "**What is spaCy in Natural Language Processing (NLP)?**\n",
        "\n",
        "\n",
        "**spaCy** is an open-source Python library specifically designed for production/enterprise-grade Natural Language Processing (NLP). It focuses on providing fast and efficient tools for various NLP tasks, making it well-suited for real-world applications and large-scale text processing.\n",
        "\n",
        "\n",
        "- Key Features and Benefits of spaCy:\n",
        "\n",
        "    * Speed and Efficiency: spaCy is known for its exceptional performance, making it ideal for handling large volumes of text data. It leverages optimized algorithms and Cython implementations for blazing-fast processing.\n",
        "    * Production-Ready: spaCy is built with a focus on practical applications and ease of deployment. Its streamlined API and well-documented functionalities simplify the development process, enabling seamless integration into existing workflows.\n",
        "    * Pre-trained Models: spaCy offers a range of pre-trained statistical models for several languages, enabling out-of-the-box capabilities such as part-of-speech tagging, named entity recognition, dependency parsing, and more.\n",
        "    * Customizability: In addition to offering pre-trained models, spaCy allows developers to train custom models for specific tasks or domains, enhancing its flexibility for specialized applications.\n",
        "    * Ecosystem and Integrations: spaCy benefits from a rich ecosystem of plugins and extensions, enabling seamless integration with other libraries and tools within the NLP and machine learning landscape.\n",
        "\n",
        "- Typical Use Cases:\n",
        "\n",
        "    * Information Extraction: spaCy excels at tasks like entity recognition, relation extraction, and fact extraction, enabling the automatic extraction of valuable insights from unstructured text data.\n",
        "    * Text Classification: spaCy can be used for sentiment analysis, topic classification, and other text classification tasks, helping categorize and understand large amounts of text.\n",
        "    * Chatbots and Conversational Agents: spaCy's capabilities can be leveraged to power chatbots and conversational agents by providing text preprocessing, intent recognition, and entity extraction functionalities.\n",
        "    * Preprocessing for Deep Learning: spaCy's efficient text preprocessing tools, including tokenization, lemmatization, and dependency parsing, make it a valuable asset for preparing text data for further analysis using deep learning models.\n",
        "\n",
        "\n",
        "\n",
        "**spaCy** is a powerful and versatile NLP library, renowned for its speed, efficiency, and production-readiness. Its wide range of features and capabilities makes it an indispensable tool for developers and practitioners working on real-world NLP applications and projects involving large-scale text data processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "# Load the spaCy model for Named Entity Recognition (NER)\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def score_sentences(sentences, freq_dist, text):\n",
        "    \"\"\"\n",
        "    Scores sentences based on multiple features to determine their importance for extractive summarization.\n",
        "\n",
        "    This function computes a score for each sentence in the text by considering various features like word frequency, \n",
        "    sentence length, sentence position, and the presence of named entities. The score is used to rank sentences \n",
        "    when generating an extractive summary.\n",
        "\n",
        "    Args:\n",
        "        sentences (list of str): A list of sentences extracted from the original text.\n",
        "        freq_dist (nltk.probability.FreqDist): A frequency distribution object containing normalized word frequencies.\n",
        "        text (str): The original text from which sentences are extracted.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary where keys are sentences and values are their corresponding scores.\n",
        "    \"\"\"\n",
        "    sentence_scores = {}  # Initialize an empty dictionary to store scores for each sentence\n",
        "    doc = nlp(text)  # Use spaCy's NLP model to analyze the text for Named Entity Recognition (NER)\n",
        "    \n",
        "    for idx, sentence in enumerate(sentences):\n",
        "        words = word_tokenize(sentence.lower())  # Tokenize the sentence into words and convert to lowercase\n",
        "        score = 0  # Initialize the score for the current sentence\n",
        "        \n",
        "        # Score based on word frequency\n",
        "        for word in words:\n",
        "            if word in freq_dist:  # If the word is in the frequency distribution\n",
        "                score += freq_dist[word]  # Add its frequency to the score\n",
        "        \n",
        "        # Feature 1: Sentence Length\n",
        "        if len(words) > 4 and len(words) < 25:  # Length threshold for a sentence to be considered \"ideal\"\n",
        "            score *= 1.2  # Boost score if the sentence length is within the desired range\n",
        "        \n",
        "        # Feature 2: Sentence Position\n",
        "        if idx < len(sentences) * 0.2:  # If the sentence is among the first 20% in the text\n",
        "            score *= 1.5  # Boost score to emphasize its importance\n",
        "        \n",
        "        # Feature 3: Named Entity Recognition (NER)\n",
        "        named_entities = [ent.text for ent in doc.ents if ent.text in sentence]  # Extract named entities from the sentence\n",
        "        if named_entities:  # If the sentence contains named entities\n",
        "            score *= 1.3  # Boost score\n",
        "        \n",
        "        sentence_scores[sentence] = score  # Store the computed score for the current sentence\n",
        "\n",
        "    return sentence_scores  # Return the dictionary containing sentences and their scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_summary(sentence_scores, num_sentences=3):\n",
        "    sorted_sentences = sorted(sentence_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    summary_sentences = [sentence[0] for sentence in sorted_sentences[:num_sentences]]\n",
        "    summary = ' '.join(summary_sentences)\n",
        "    return summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enhanced/improved Summary:\n",
            "Once this computer program was up and running, Lorenz could produce long-term forecasts by feeding the predicted weather back into the computer over and over again, with each run forecasting further into the future.Accurate minute-by-minute forecasts added up into days, and then weeks. In the interests of saving time he decided not to start from scratch; instead he took the computer’s prediction from halfway through the first run and used that as the starting point. The best we can do for three bodies is to predict their movements moment by moment, and feed those predictions back into our equations …\n",
            "Though the dance of the planets has a lengthy prediction horizon, the effects of chaos cannot be ignored, for the intricate interplay of gravitation tugs among the planets has a large influence on the trajectories of the asteroids.\n"
          ]
        }
      ],
      "source": [
        "sentences, filtered_words = preprocess_text(text)\n",
        "freq_dist = compute_word_frequency(filtered_words)\n",
        "sentence_scores = score_sentences(sentences, freq_dist, text)\n",
        "summary = generate_summary(sentence_scores)\n",
        "\n",
        "print(\"Enhanced/improved Summary:\")\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Enhanced/improved Summary:\n",
        "    Once this computer program was up and running, Lorenz could produce long-term forecasts by feeding the predicted weather back into the computer over and over again, with \n",
        "    each run forecasting further into the future.Accurate minute-by-minute forecasts added up into days, and then weeks. In the interests of saving time he decided not to start \n",
        "    from scratch; instead he took the computer’s prediction from halfway through the first run and used that as the starting point. The best we can do for three bodies is to \n",
        "    predict their movements moment by moment, and feed those predictions back into our equations …\n",
        "\n",
        "    Though the dance of the planets has a lengthy prediction horizon, the effects of chaos cannot be ignored, for the intricate interplay of gravitation tugs among the planets \n",
        "    has a large influence on the trajectories of the asteroids.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The enhanced/improved summary is better than the previous one, but it still has some shortcomings:\n",
        "\n",
        "- Positives:\n",
        "\n",
        "    * Includes more context: It now mentions Lorenz and his experiment with weather prediction, providing some context for the discussion of chaos theory.\n",
        "    * Slightly better flow: The sentences are somewhat more connected than in the previous summary.\n",
        "\n",
        "- Negatives:\n",
        "\n",
        "    * Still incomplete: While it touches on Lorenz's experiment and the three-body problem, it still misses the core concept of chaos theory – the idea that tiny differences in initial conditions can lead to drastically different outcomes over time. The terms \"butterfly effect\" and \"prediction horizon\" are still absent.\n",
        "    * Lacks focus: The summary jumps between weather prediction, the three-body problem, and the solar system without clearly connecting these ideas.\n",
        "    * Some sentences are still out of context: The sentence \"In the interests of saving time he decided not to start from scratch; instead he took the computer’s prediction from halfway through the first run and used that as the starting point\" is included without explaining its significance in the context of Lorenz's discovery.   \n",
        "\n",
        "\n",
        "Overall, the enhanced summary is an improvement, but it still needs work to be considered a good summary of the original text. It needs to be more focused, include the key concepts of chaos theory, and provide a clear and logical flow of ideas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets incooporate some Evaluation: \n",
        "\n",
        "To evaluate the quality of the summaries generated by our extractive summarization algorithm, we can use several evaluation metrics. The most widely used evaluation metric for summarization tasks is ROUGE (Recall-Oriented Understudy for Gisting Evaluation). Other evaluation metrics include BLEU (Bilingual Evaluation Understudy) and METEOR (Metric for Evaluation of Translation with Explicit ORdering).\n",
        "\n",
        "Evaluation Metrics for Summarization\n",
        "- ROUGE (Recall-Oriented Understudy for Gisting Evaluation):\n",
        "    * ROUGE-1: Measures the overlap of unigrams (single words) between the generated summary and a reference summary.\n",
        "    * ROUGE-2: Measures the overlap of bigrams (two consecutive words).\n",
        "    * ROUGE-L: Measures the longest common subsequence (LCS) between the generated and reference summaries, capturing the in-sequence overlap.\n",
        "- BLEU (Bilingual Evaluation Understudy):\n",
        "    * Primarily used for machine translation but can be adapted for summarization.\n",
        "    * Measures n-gram precision of a generated text concerning one or more reference texts.\n",
        "- METEOR (Metric for Evaluation of Translation with Explicit ORdering):\n",
        "    * Designed to improve BLEU by addressing problems like synonymy and stemming.\n",
        "    * It considers unigram matches between generated and reference summaries, applying stemming and synonymy matching.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting absl-py (from rouge-score)\n",
            "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: nltk in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from rouge-score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from rouge-score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: click in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from nltk->rouge-score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from nltk->rouge-score) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from nltk->rouge-score) (2023.10.3)\n",
            "Requirement already satisfied: tqdm in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from nltk->rouge-score) (4.65.0)\n",
            "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24932 sha256=590cf4daae0c0d619f2feb677ab7699f688115d174b8d063549de34e57a72b57\n",
            "  Stored in directory: /Users/krusere/Library/Caches/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: absl-py, rouge-score\n",
            "Successfully installed absl-py-2.1.0 rouge-score-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF6iu7wpcic0"
      },
      "source": [
        "#### **Conclusion**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNtVja6Mcmum"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 ('capstone')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "6c67f8adbebdea49f93c63d2a2dcd5cb4fed3a722249876be11f90d0fd0a91fc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
